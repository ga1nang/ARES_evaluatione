{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa2d6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/pc1/Ubuntu/Extend_Data/anaconda3/envs/ares_thanh/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM not imported.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Now import\n",
    "from ares import ARES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: 'assigned_batch_size' not provided for classifier_model, using default value 1.\n",
      "\n",
      "Warning: 'gradient_accumulation_multiplier' not provided for classifier_model, using default value 32.\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/218 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 218/218 [00:00<00:00, 2384.66 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 2153.44 examples/s]\n",
      "Map: 100%|██████████| 29/29 [00:00<00:00, 2087.72 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model\n",
      "Checkpoint Path: checkpoints/microsoft-deberta-v3-large/Context_Relevance_Label_None_2025-06-11_00:48:29.pt\n",
      "Beginning Training\n",
      "Current Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218/218 [00:48<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0/10] train_loss: 0.69263 valid_loss: 0.68526\n",
      "Validation loss decreased (inf --> 0.685256).  Saving model ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "File checkpoints/microsoft-deberta-v3-large/Context_Relevance_Label_None_2025-06-11_00:48:29.pt cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m classifier_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets_file/output/Train_Context_Relevance_Data.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_set\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets_file/output/Test_Context_Relevance_Data.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5e-6\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[1;32m     11\u001b[0m ares \u001b[38;5;241m=\u001b[39m ARES(classifier_model\u001b[38;5;241m=\u001b[39mclassifier_config)\n\u001b[0;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mares\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/em_Thanh/Evaluation/ARES_evaluatione/ares/ares.py:139\u001b[0m, in \u001b[0;36mARES.train_classifier\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping binary classifier configuration due to missing parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[43mbinary_classifer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier_model_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/em_Thanh/Evaluation/ARES_evaluatione/ares/binary_classifier.py:90\u001b[0m, in \u001b[0;36mbinary_classifer_config\u001b[0;34m(training_dataset, validation_set, label_column, num_epochs, patience_value, learning_rate, training_dataset_path, validation_dataset_path, model_choice, assigned_batch_size, gradient_accumulation_multiplier, number_of_runs, num_warmup_steps, training_row_limit, validation_row_limit)\u001b[0m\n\u001b[1;32m     73\u001b[0m tokenized_datasets \u001b[38;5;241m=\u001b[39m initalize_dataset_for_tokenization(tokenizer, training_dataset_arrow, validation_dataset_arrow, test_dataset_arrow)\n\u001b[1;32m     75\u001b[0m train_and_eval_settings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber_of_runs\u001b[39m\u001b[38;5;124m\"\u001b[39m: number_of_runs,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenized_datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenized_datasets,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_accumulation_multiplier\u001b[39m\u001b[38;5;124m\"\u001b[39m: gradient_accumulation_multiplier\n\u001b[1;32m     88\u001b[0m }\n\u001b[0;32m---> 90\u001b[0m model, avg_train_losses, avg_valid_losses, eval_dataloader, inference_times \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_and_eval_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m total_predictions, total_references, metric \u001b[38;5;241m=\u001b[39m evaluate_model(model, model_choice, checkpoint_path, device, eval_dataloader, inference_times)\n\u001b[1;32m     94\u001b[0m print_and_save_model(total_predictions, total_references, checkpoint_path, metric)\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/em_Thanh/Evaluation/ARES_evaluatione/ares/LLM_as_a_Judge_Adaptation/General_Binary_Classifier.py:856\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    854\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    855\u001b[0m valid_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 856\u001b[0m \u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m early_stopping\u001b[38;5;241m.\u001b[39mearly_stop:\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/em_Thanh/Evaluation/ARES_evaluatione/ares/LLM_as_a_Judge_Adaptation/pytorchtools.py:35\u001b[0m, in \u001b[0;36mEarlyStopping.__call__\u001b[0;34m(self, val_loss, model)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score \u001b[38;5;241m=\u001b[39m score\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_score \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/em_Thanh/Evaluation/ARES_evaluatione/ares/LLM_as_a_Judge_Adaptation/pytorchtools.py:50\u001b[0m, in \u001b[0;36mEarlyStopping.save_checkpoint\u001b[0;34m(self, val_loss, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_func(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation loss decreased (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss_min\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).  Saving model ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loss_min \u001b[38;5;241m=\u001b[39m val_loss\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/anaconda3/envs/ares_thanh/lib/python3.9/site-packages/torch/serialization.py:964\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    961\u001b[0m     f \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(f)\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    965\u001b[0m         _save(\n\u001b[1;32m    966\u001b[0m             obj,\n\u001b[1;32m    967\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    970\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    971\u001b[0m         )\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/anaconda3/envs/ares_thanh/lib/python3.9/site-packages/torch/serialization.py:828\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 828\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/pc1/Ubuntu/Extend_Data/anaconda3/envs/ares_thanh/lib/python3.9/site-packages/torch/serialization.py:792\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    786\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, get_crc32_options(), _get_storage_alignment()\n\u001b[1;32m    788\u001b[0m         )\n\u001b[1;32m    789\u001b[0m     )\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m--> 792\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_crc32_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_storage_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: File checkpoints/microsoft-deberta-v3-large/Context_Relevance_Label_None_2025-06-11_00:48:29.pt cannot be opened."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "classifier_config = {\n",
    "    \"training_dataset\": [\"../datasets_file/output/Train_Context_Relevance_Data.tsv\"],\n",
    "    \"validation_set\": [\"../datasets_file/output/Test_Context_Relevance_Data.tsv\"], \n",
    "    \"label_column\": [\"Context_Relevance_Label\"], \n",
    "    \"model_choice\": \"microsoft/deberta-v3-large\", # Default model is \"microsoft/deberta-v3-large\"\n",
    "    \"num_epochs\": 10, \n",
    "    \"patience_value\": 3, \n",
    "    \"learning_rate\": 5e-6\n",
    "}\n",
    "\n",
    "ares = ARES(classifier_model=classifier_config)\n",
    "results = ares.train_classifier()\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef9947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ares_thanh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
