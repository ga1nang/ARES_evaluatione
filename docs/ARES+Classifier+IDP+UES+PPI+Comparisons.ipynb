{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>ARES Evaluation Strategies</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Install Dependencies <p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ares-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IDP + UES</h2>\n",
    "<p>Uses targeted prompts to enable pre-trained models to assess content relevance and accuracy in a zero-shot manner.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "ues_idp_config = {\n",
    "    # Dataset for in-domain prompts\n",
    "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
    "    \n",
    "    # Dataset for unlabeled evaluation\n",
    "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n",
    "    \n",
    "    # Model: GPT-3.5\n",
    "    \"model_choice\" : \"gpt-3.5-turbo-0125\"\n",
    "} \n",
    "\n",
    "# Optional: Provide an alternative model of your choice below.\n",
    "# Here are some models you can choose from:\n",
    "# - mistralai/Mistral-7B-Instruct-v0.2\n",
    "# - mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "# - gpt-4-turbo-preview\n",
    "# - microsoft/deberta-v3-large\n",
    "# - openlm-research/open_llama_7b_v2\n",
    "# - mosaicml/mpt-7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(ues_idp=ues_idp_config)\n",
    "results = ares.ues_idp()\n",
    "print(results)\n",
    "\n",
    "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training Classifier + IDP + UES</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "ues_idp_config = {\n",
    "    # Dataset for in-domain prompts\n",
    "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
    "    \n",
    "    # Dataset for unlabeled evaluation\n",
    "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n",
    "    \n",
    "    # Model: GPT-3.5\n",
    "    \"model_choice\" : \"gpt-3.5-turbo-0125\"\n",
    "} \n",
    "\n",
    "# Training Classifier\n",
    "classifier_config = {\n",
    "    \"training_dataset\": [\"nq_synth_queries.tsv\"], \n",
    "    \"validation_set\": [\"nq_ratio_0.7.tsv\"], \n",
    "    \"label_column\": [\"Context_Relevance_Label\"], \n",
    "    \"num_epochs\": 10, \n",
    "    \"patience_value\": 3, \n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"assigned_batch_size\": 1,  \n",
    "    \"gradient_accumulation_multiplier\": 32,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares_module = ARES(classifier_model=classifier_config)\n",
    "results = ares_module.train_classifier()\n",
    "print(results)\n",
    "\n",
    "# Trains and saves checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(ues_idp=ues_idp_config)\n",
    "results = ares.ues_idp()\n",
    "print(results)\n",
    "\n",
    "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training Classifier + PPI + UES</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>UES</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "ues_idp_config = {\n",
    "    # Dataset for in-domain prompts\n",
    "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
    "    \n",
    "    # Dataset for unlabeled evaluation\n",
    "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n",
    "    \n",
    "    # Default model choice\n",
    "    \"model_choice\" : \"gpt-3.5-turbo-1106\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(ues_idp=ues_idp_config)\n",
    "results = ares.ues_idp()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training Classifier</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Generates checkpoint which is used in PPI below</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "classifier_config = {\n",
    "    \"training_dataset\": [\"nq_synth_queries.tsv\"], \n",
    "    \"validation_set\": [\"nq_ratio_0.7.tsv\"], \n",
    "    \"label_column\": [\"Context_Relevance_Label\"], \n",
    "    \"num_epochs\": 10, \n",
    "    \"patience_value\": 3, \n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"assigned_batch_size\": 1,  \n",
    "    \"gradient_accumulation_multiplier\": 32,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(classifier_model=classifier_config)\n",
    "results = ares.train_classifier()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PPI</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "ppi_config = { \n",
    "    \"evaluation_datasets\": [\"nq_unlabeled_output.tsv\"], \n",
    "    \"checkpoints\": [\"Context_Relevance_Label_joint_trained_date_time.pt\"], \n",
    "    \"labels\": [\"Context_Relevance_Label\"], \n",
    "    \"rag_type\": \"question_answering\", \n",
    "    \"gold_label_paths\": [\"nq_labeled_output.tsv\"],\n",
    "    \"prediction_filepaths\": [\"nq_0.6_predictions_updated.tsv\"],\n",
    "}\n",
    "\n",
    "# Install checkpoint here!\n",
    "# Context Relevance: https://drive.google.com/file/d/1SK4THhBlyXrwxf7v3s2SAd05ZclLY5FF/view?usp=sharing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(ppi=ppi_config)\n",
    "results = ares.evaluate_RAG()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ARES Comparison to RAGAS and Zeroshot Llama</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ARES Configuration</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Synthetic Generator</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "    \n",
    "synth_config = { \n",
    "    \"document_filepaths\": [\"nq_labeled_output.tsv\"] ,\n",
    "    \"few_shot_prompt_filename\": [\"nq_few_shot_prompt_for_synthetic_query_generation.tsv\"],\n",
    "    \"synthetic_queries_filenames\": [\"nq_synthetic_queries.tsv\"], \n",
    "    \"documents_sampled\": 6189\n",
    "}\n",
    "\n",
    "ares_module = ARES(synthetic_query_generator=synth_config)  \n",
    "results = ares_module.generate_synthetic_data()\n",
    "print(results)\n",
    "\n",
    "# Generates and saves synthetic queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Training Classifier</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "classifier_config = {\n",
    "    \"training_dataset\": [\"nq_synth_queries.tsv\"], \n",
    "    \"validation_set\": [\"nq_ratio_0.7.tsv\"], \n",
    "    \"label_column\": [\"Context_Relevance_Label\", \"Answer_Relevance_Label\"], \n",
    "    \"num_epochs\": 10, \n",
    "    \"patience_value\": 3, \n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"assigned_batch_size\": 1,  \n",
    "    \"gradient_accumulation_multiplier\": 32,  \n",
    "}\n",
    "\n",
    "ares = ARES(classifier_model=classifier_config)\n",
    "results = ares.train_classifier()\n",
    "print(results)\n",
    "\n",
    "# Trains and saves classifier for context relevance and answer relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>PPI</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_Relevance_Label Scoring\n",
      "ARES Prediction: [0.6056978059262574]\n",
      "ARES Confidence Interval: [[0.547, 0.664]]\n",
      "Number of Examples in Evaluation Set: [4421]\n",
      "Ground Truth Performance: [0.6]\n",
      "ARES LLM Judge Accuracy on Ground Truth Labels: [0.789]\n",
      "Annotated Examples used for PPI: 300\n",
      "------------\n",
      "Answer_Relevance_Label Scoring\n",
      "ARES Prediction: [0.595499509914802]\n",
      "ARES Confidence Interval: [[0.541, 0.65]]\n",
      "Number of Examples in Evaluation Set: [4421]\n",
      "Ground Truth Performance: [0.6]\n",
      "ARES LLM Judge Accuracy on Ground Truth Labels: [0.629]\n",
      "Annotated Examples used for PPI: 300\n"
     ]
    }
   ],
   "source": [
    "from ares import ARES\n",
    "\n",
    "ppi_config = { \n",
    "    \"evaluation_datasets\": ['nq_unlabeled_output.tsv'], \n",
    "    \"checkpoints\": [\"Context_Relevance_Label_joint_trained_date_time.pt\", \"Answer_Relevance_Label_joint_trained_date_time.pt\"], \n",
    "    \"rag_type\": \"question_answering\", \n",
    "    \"labels\": [\"Context_Relevance_Label\", \"Answer_Relevance_Label\"], \n",
    "    \"gold_label_path\": \"nq_labeled_output.tsv\", \n",
    "}\n",
    "\n",
    "ares_module = ARES(ppi=ppi_config)\n",
    "results = ares_module.evaluate_RAG()\n",
    "print(results)\n",
    "\n",
    "# Download checkpoints here! \n",
    "\n",
    "# Context Relevance: https://drive.google.com/file/d/1SK4THhBlyXrwxf7v3s2SAd05ZclLY5FF/view?usp=sharing\n",
    "# Answer Relevance: https://drive.google.com/file/d/1yg1q6WrCwq7q07YceZUsd7FLVuLNJEue/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>RAGAS Configuration</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Data Cleaning | Context Relevance Label Filter</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "def load_and_prepare_dataset(file_path):\n",
    "    # Load the dataset from the TSV file\n",
    "    dataset_df = pd.read_csv(file_path, delimiter='\\t')\n",
    "    \n",
    "    # Remove rows where 'Context_Relevance_Label' has no values\n",
    "    dataset_df = dataset_df.dropna(subset=['Context_Relevance_Label'])\n",
    "    \n",
    "    # Convert 'Context_Relevance_Label' to string if it is not already\n",
    "    dataset_df['Context_Relevance_Label'] = dataset_df['Context_Relevance_Label'].astype(str)\n",
    "    \n",
    "    # Use 'Context_Relevance_Label' as 'ground_truth'\n",
    "    prepared_data = {\n",
    "        'question': dataset_df['Query'].tolist(),\n",
    "        'contexts': [[doc] for doc in dataset_df['Document'].tolist()],  # Contexts are expected to be list of lists\n",
    "        'answer': dataset_df['Answer'].tolist(),\n",
    "        'ground_truth': dataset_df['Context_Relevance_Label'].tolist(),  # Using 'Context_Relevance_Label' as 'ground_truth'\n",
    "    }\n",
    "    \n",
    "    # Convert to HuggingFace's Dataset format\n",
    "    dataset = Dataset.from_dict(prepared_data)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> ARES Label Filter: Removes rows w/ no values for specified label</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Context Relevance Accuracy</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 8842/8842 [12:15<00:00, 12.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_precision': 0.5549, 'context_recall': 0.4737}\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import context_recall, context_precision\n",
    "\n",
    "# Load and prepare the dataset\n",
    "file_path = \"nq_unlabeled_output.tsv\"  # Update with the actual file path\n",
    "prepared_dataset = load_and_prepare_dataset(file_path)\n",
    "\n",
    "# Specify metrics\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "\n",
    "result = evaluate(prepared_dataset, metrics=metrics)  # Pass the initialized llm\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Data Cleaning | Answer Relevance Label Filter</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_dataset(file_path):\n",
    "    # Load the dataset from the TSV file\n",
    "    dataset_df = pd.read_csv(file_path, delimiter='\\t')\n",
    "\n",
    "    dataset_df = dataset_df.dropna(subset=['Answer_Relevance_Label'])\n",
    "    \n",
    "    # Convert 'Context_Relevance_Label' to string if it is not already\n",
    "    dataset_df['Answer_Relevance_Label'] = dataset_df['Answer_Relevance_Label'].astype(str)\n",
    "    \n",
    "    # Use 'Context_Relevance_Label' as 'ground_truth'\n",
    "    prepared_data = {\n",
    "        'question': dataset_df['Query'].tolist(),\n",
    "        'contexts': [[doc] for doc in dataset_df['Document'].tolist()], \n",
    "        'answer': dataset_df['Answer'].tolist(),\n",
    "        'ground_truth': dataset_df['Answer_Relevance_Label'].tolist(), \n",
    "    }\n",
    "    \n",
    "    # Convert to HuggingFace's Dataset format\n",
    "    dataset = Dataset.from_dict(prepared_data)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  92%|█████████▏| 4054/4421 [23:01<02:25,  2.52it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  92%|█████████▏| 4063/4421 [23:04<02:01,  2.95it/s]Failed to parse output. Returning None.\n",
      "Evaluating: 100%|██████████| 4421/4421 [25:03<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': 0.7511}\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy\n",
    "\n",
    "file_path = \"nq_unlabeled_output.tsv\" \n",
    "prepared_dataset = load_and_prepare_dataset(file_path)\n",
    "\n",
    "# Specify metrics\n",
    "metrics = [\n",
    "    answer_relevancy\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate(prepared_dataset, metrics=metrics) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Zeroshot Llama Configuration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "\n",
    "\n",
    "ues_idp_config = {\n",
    "    # Dataset for in-domain prompts\n",
    "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
    "    \n",
    "    # Dataset for unlabeled evaluation\n",
    "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n",
    "    \n",
    "    # Model: Mistral 7B\n",
    "    \"model_choice\" : \"codellama/CodeLlama-13b-Instruct-hf\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(ues_idp=ues_idp_config)\n",
    "results = ares.ues_idp()\n",
    "print(results)\n",
    "\n",
    "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Zeroshot Mistral Configuration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ares import ARES\n",
    "import os\n",
    "\n",
    "ues_idp_config = {\n",
    "    # Dataset for in-domain prompts\n",
    "    \"in_domain_prompts_dataset\": \"nq_few_shot_prompt_for_judge_scoring.tsv\",\n",
    "    \n",
    "    # Dataset for unlabeled evaluation\n",
    "    \"unlabeled_evaluation_set\": \"nq_unlabeled_output.tsv\", \n",
    "    \n",
    "    # Model: Mistral 7B\n",
    "    \"model_choice\" : \"mistralai/Mixtral-8x7B-v0.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ares = ARES(ues_idp=ues_idp_config)\n",
    "results = ares.ues_idp()\n",
    "print(results)\n",
    "\n",
    "# {'Context Relevance Scores': [Score], 'Answer Faithfulness Scores': [Score], 'Answer Relevance Scores': [Score]}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_judge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
